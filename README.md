<h1 align="center">ðŸ¤–âœ¨ Awesome Repository-Level Code Generation âœ¨ðŸ¤–</h1>

<p align="center">
  <a href="https://awesome.re"><img src="https://awesome.re/badge.svg"></a>
  <img src="https://img.shields.io/github/stars/YerbaPage/Awesome-Repo-Level-Code-Generation?color=yellow&label=â­ Stars">
  <img src="https://img.shields.io/badge/PRs-Welcome-red">
  <img src="https://img.shields.io/github/last-commit/YerbaPage/Awesome-Repo-Level-Code-Generation?label=â° Last%20Commit">
</p>

ðŸŒŸ A curated list of awesome repository-level code generation research papers and resources. If you want to contribute to this list (please do), feel free to send me a pull request. ðŸš€

## ðŸ“š Contents

- [ðŸ“š Contents](#-contents)
- [ðŸ’¥ Repo-Level Issue Resolution](#-repo-level-issue-resolution)
- [ðŸ¤– Repo-Level Code Completion](#-repo-level-code-completion)
- [ðŸ“Š Datasets and Benchmarks](#-datasets-and-benchmarks)

## ðŸ’¥ Repo-Level Issue Resolution

- Unveiling Pitfalls: Understanding Why AI-driven Code Agents Fail at GitHub Issue Resolution [2025-03-arXiv] [[ðŸ“„ paper](https://arxiv.org/pdf/2503.12374)]

- LocAgent: Graph-Guided LLM Agents for Code Localization [2025-03-arXiv] [[ðŸ“„ paper](https://arxiv.org/pdf/2503.09089)] [[ðŸ”— repo](https://github.com/gersteinlab/LocAgent)]

- SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution [2025-02-arXiv] [[ðŸ“„ paper](https://arxiv.org/pdf/2502.18449)] [[ðŸ”— repo](https://github.com/facebookresearch/swe-rl)]

- SWE-Lancer: Can Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering? [2025-arXiv] [[ðŸ“„ paper](https://arxiv.org/pdf/2502.12115)] [[ðŸ”— repo](https://github.com/openai/SWELancer-Benchmark)]

- Evaluating Agent-based Program Repair at Google [2025-01-arXiv] [[ðŸ“„ paper](https://arxiv.org/pdf/2501.07531)]

- SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub Issue Resolution [2025-01-arXiv] [[ðŸ“„ paper](https://arxiv.org/pdf/2501.05040)] [[ðŸ”— repo](https://github.com/InternLM/SWE-Fixer)]

- Training Software Engineering Agents and Verifiers with SWE-Gym [2024-12-arXiv] [[ðŸ“„ paper](https://arxiv.org/pdf/2412.21139)] [[ðŸ”— repo](https://github.com/SWE-Gym/SWE-Gym)]

- CODEV: Issue Resolving with Visual Data [2024-12-arXiv] [[ðŸ“„ paper](https://arxiv.org/pdf/2412.17315)] [[ðŸ”— repo](https://github.com/luolin101/CodeV)]

- LLMs as Continuous Learners: Improving the Reproduction of Defective Code in Software Issues [2024-11-arXiv] [[ðŸ“„ paper](https://arxiv.org/pdf/2411.13941)]

- Globant Code Fixer Agent Whitepaper [2024-11] [[ðŸ“„ paper](https://ai.globant.com/wp-content/uploads/2024/11/Whitepaper-Globant-Code-Fixer-Agent.pdf)]

- MarsCode Agent: AI-native Automated Bug Fixing [2024-11-arXiv] [[ðŸ“„ paper](https://arxiv.org/html/2411.10213v1)]

- Lingma SWE-GPT: An Open Development-Process-Centric Language Model for Automated Software Improvement [2024-11-arXiv] [[ðŸ“„ paper](https://arxiv.org/html/2411.00622v1)] [[ðŸ”— repo](https://github.com/LingmaTongyi/Lingma-SWE-GPT)]

- SWE-Search: Enhancing Software Agents with Monte Carlo Tree Search and Iterative Refinement [2024-10-arXiv] [[ðŸ“„ paper](https://arxiv.org/pdf/2410.20285)] [[ðŸ”— repo](https://github.com/aorwall/moatless-tree-search)]

- AutoCodeRover: Autonomous Program Improvement [2024-09-ISSTA] [[ðŸ“„ paper](https://dl.acm.org/doi/10.1145/3650212.3680384)] [[ðŸ”— repo](https://github.com/nus-apr/auto-code-rover)]

- SpecRover: Code Intent Extraction via LLMs [2024-08-arXiv] [[ðŸ“„ paper](https://arxiv.org/abs/2408.02232)]

- OpenHands: An Open Platform for AI Software Developers as Generalist Agents [2024-07-arXiv] [[ðŸ“„ paper](https://arxiv.org/abs/2407.16741)] [[ðŸ”— repo](https://github.com/All-Hands-AI/OpenHands)]

- AGENTLESS: Demystifying LLM-based Software Engineering Agents [2024-07-arXiv] [[ðŸ“„ paper](https://arxiv.org/abs/2407.01489)]

- RepoGraph: Enhancing AI Software Engineering with Repository-level Code Graph [2024-07-arXiv] [[ðŸ“„ paper](https://arxiv.org/abs/2407.01489)] [[ðŸ”— repo](https://github.com/ozyyshr/RepoGraph)]

- How to Understand Whole Software Repository? [2024-06-arXiv] [[ðŸ“„ paper](https://arxiv.org/pdf/2406.01422)]

- SWE-Agent: Can Language Models Resolve Real-World GitHub Issues? [2024-01-ICLR] [[ðŸ“„ paper](https://openreview.net/forum?id=VTF8yNQM66)] [[ðŸ”— repo](https://github.com/princeton-nlp/SWE-bench)]

## ðŸ¤– Repo-Level Code Completion

- Improving FIM Code Completions via Context & Curriculum Based Learning [2024-12-arXiv] [[ðŸ“„ paper](https://arxiv.org/abs/2412.16589)]

- ContextModule: Improving Code Completion via Repository-level Contextual Information [2024-12-arXiv] [[ðŸ“„ paper](https://arxiv.org/abs/2412.08063)]

- RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal Reinforcement and Retrieval-Augmented Generation [2024-09-arXiv] [[ðŸ“„ paper](https://arxiv.org/abs/2409.13122)]
  
- RAMBO: Enhancing RAG-based Repository-Level Method Body Completion [2024-09-arXiv] [[ðŸ“„ paper](https://arxiv.org/abs/2409.15204)] [[ðŸ”— repo](https://github.com/ise-uet-vnu/rambo)]
  
- RLCoder: Reinforcement Learning for Repository-Level Code Completion [2024-07-arXiv] [[ðŸ“„ paper](https://arxiv.org/abs/2407.19487)] [[ðŸ”— repo](https://github.com/DeepSoftwareAnalytics/RLCoder)]
  
- Hierarchical Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs [2024-06-arXiv] [[ðŸ“„ paper](https://arxiv.org/abs/2406.18294)] [[ðŸ”— repo](https://github.com/Hambaobao/HCP-Coder)]
  
- STALL+: Boosting LLM-based Repository-level Code Completion with Static Analysis [2024-06-arXiv] [[ðŸ“„ paper](https://arxiv.org/abs/2406.10018)]
  
- GraphCoder: Enhancing Repository-Level Code Completion via Code Context Graph-based Retrieval and Language Model [2024-06-arXiv] [[ðŸ“„ paper](https://arxiv.org/abs/2406.07003)]
  
- Enhancing Repository-Level Code Generation with Integrated Contextual Information [2024-06-arXiv] [[ðŸ“„ paper](https://arxiv.org/pdf/2406.03283)]
  
- R2C2-Coder: Enhancing and Benchmarking Real-world Repository-level Code Completion Abilities of Code Large Language Models [2024-06-arXiv] [[ðŸ“„ paper](https://arxiv.org/abs/2406.01359)]
  
- Natural Language to Class-level Code Generation by Iterative Tool-augmented Reasoning over Repository [2024-05-arXiv] [[ðŸ“„ paper](https://arxiv.org/abs/2405.01573)] [[ðŸ”— repo](https://github.com/microsoft/repoclassbench)]
  
- Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback [2024-03-arXiv] [[ðŸ“„ paper](https://arxiv.org/abs/2403.16792)] [[ðŸ”— repo](https://github.com/CGCL-codes/naturalcc/tree/main/examples/cocogen)]
  
- Repoformer: Selective Retrieval for Repository-Level Code Completion [2024-03-arXiv] [[ðŸ“„ paper](https://arxiv.org/abs/2403.10059)] [[ðŸ”— repo](https://repoformer.github.io/)]
  
- RepoHyper: Search-Expand-Refine on Semantic Graphs for Repository-Level Code Completion [2024-03-arXiv] [[ðŸ“„ paper](https://arxiv.org/abs/2403.06095)] [[ðŸ”— repo](https://github.com/FSoft-AI4Code/RepoHyper)]
  
- RepoMinCoder: Improving Repository-Level Code Generation Based on Information Loss Screening [2024-07-Internetware] [[ðŸ“„ paper](https://dl.acm.org/doi/10.1145/3671016.3674819)]
  
- CodePlan: Repository-Level Coding using LLMs and Planning [2024-07-FSE] [[ðŸ“„ paper](https://dl.acm.org/doi/abs/10.1145/3643757)] [[ðŸ”— repo](https://github.com/microsoft/codeplan)]

- RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation [2023-10-EMNLP] [[ðŸ“„ paper](https://aclanthology.org/2023.emnlp-main.151/)] [[ðŸ”— repo](https://github.com/microsoft/CodeT/tree/main/RepoCoder)]
  
- Monitor-Guided Decoding of Code LMs with Static Analysis of Repository Context [2023-09-NeurIPS] [[ðŸ“„ paper](https://neurips.cc/virtual/2023/poster/70362)] [[ðŸ”— repo](https://aka.ms/monitors4codegen)]
  
- RepoFusion: Training Code Models to Understand Your Repository [2023-06-arXiv] [[ðŸ“„ paper](https://arxiv.org/abs/2306.10998)] [[ðŸ”— repo](https://github.com/ServiceNow/RepoFusion)]
  
- Repository-Level Prompt Generation for Large Language Models of Code [2023-06-ICML] [[ðŸ“„ paper](https://proceedings.mlr.press/v202/shrivastava23a.html)] [[ðŸ”— repo](https://github.com/shrivastavadisha/repo_level_prompt_generation)]
  
- Fully Autonomous Programming with Large Language Models [2023-06-GECCO] [[ðŸ“„ paper](https://dl.acm.org/doi/pdf/10.1145/3583131.3590481)] [[ðŸ”— repo](https://github.com/KoutchemeCharles/aied2023)]

## ðŸ“Š Datasets and Benchmarks

- **SWEE-Bench & SWA-Bench**: Automated Benchmark Generation for Repository-Level Coding Tasks [2025-03-arXiv] [[ðŸ“„ paper](https://arxiv.org/pdf/2503.07701)]

- **ProjectEval**: A Benchmark for Programming Agents Automated Evaluation on Project-Level Code Generation [2025-03-arXiv] [[ðŸ“„ paper](https://arxiv.org/pdf/2503.07010)]

- **REPOST-TRAIN**: Scalable Repository-Level Coding Environment Construction with Sandbox Testing [2025-03-arXiv] [[ðŸ“„ paper](https://arxiv.org/pdf/2503.07358)] [[ðŸ”— repo](https://github.com/yiqingxyq/RepoST)]

- **Loc-Bench**: Graph-Guided LLM Agents for Code Localization [2025-03-arXiv] [[ðŸ“„ paper](https://arxiv.org/pdf/2503.09089)] [[ðŸ”— repo](https://github.com/gersteinlab/LocAgent)]

- **SWE-Lancer**: Can Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering? [2025-02-arXiv] [[ðŸ“„ paper](https://arxiv.org/pdf/2502.12115)] [[ðŸ”— repo](https://github.com/openai/SWELancer-Benchmark)]

- **HumanEvo**: An Evolution-aware Benchmark for More Realistic Evaluation of Repository-level Code Generation [2025-ICSE] [[ðŸ“„ paper](https://www.computer.org/csdl/proceedings-article/icse/2025/056900a764/251mHzzKizu)] [[ðŸ”— repo](https://github.com/DeepSoftwareAnalytics/HumanEvo)]

- **RepoExec**: On the Impacts of Contexts on Repository-Level Code Generation [2025-NAACL] [[ðŸ“„ paper](https://arxiv.org/abs/2406.11927)] [[ðŸ”— repo](https://github.com/FSoft-AI4Code/RepoExec)]

- **SWE-Gym**: Training Software Engineering Agents and Verifiers with SWE-Gym [2024-12-arXiv] [[ðŸ“„ paper](https://arxiv.org/pdf/2412.21139)] [[ðŸ”— repo](https://github.com/SWE-Gym/SWE-Gym)]

- **Visual SWE-bench**: Issue Resolving with Visual Data [2024-12-arXiv] [[ðŸ“„ paper](https://arxiv.org/pdf/2412.17315)] [[ðŸ”— repo](https://github.com/luolin101/CodeV)]

- **ExecRepoBench**: Multi-level Executable Code Completion Evaluation [2024-12-arXiv] [[ðŸ“„ paper](https://arxiv.org/abs/2412.11990)] [[ðŸ”— site](https://execrepobench.github.io/)]

- **LibEvolutionEval**: A Benchmark and Study for Version-Specific Code Generation [2024-12-arXiv] [[ðŸ“„ paper](https://arxiv.org/abs/2412.04478)]

- **REPOCOD**: Can Language Models Replace Programmers? REPOCOD Says 'Not Yet' [2024-10-arXiv] [[ðŸ“„ paper](https://arxiv.org/abs/2410.21647)] [[ðŸ”— repo](https://github.com/lt-asset/REPOCOD)]

- **M2RC-EVAL**: M2rc-Eval: Massively Multilingual Repository-level Code Completion Evaluation [2024-10-arXiv] [[ðŸ“„ paper](https://arxiv.org/abs/2410.21157)] [[ðŸ”— repo](https://github.com/M2RC-Eval-Team/M2RC-Eval)]

- **SWE-bench+**: Enhanced Coding Benchmark for LLMs [2024-10-arXiv] [[ðŸ“„ paper](https://arxiv.org/pdf/2410.06992)]

- **SWE-bench Multimodal**: Multimodal Software Engineering Benchmark [2024-10-arXiv] [[ðŸ“„ paper](https://arxiv.org/abs/2410.03859)] [[ðŸ”— site](https://swebench.com/multimodal)]

- **Codev-Bench**: How Do LLMs Understand Developer-Centric Code Completion? [2024-10-arXiv] [[ðŸ“„ paper](https://arxiv.org/abs/2410.01353)] [[ðŸ”— repo](https://github.com/LingmaTongyi/Codev-Bench)]

- **CodeRAG-Bench**: Can Retrieval Augment Code Generation? [2024-06-arXiv] [[ðŸ“„ paper](http://arxiv.org/abs/2406.14497)] [[ðŸ”— repo](https://github.com/code-rag-bench/code-rag-bench/tree/main)]

- **R2C2-Bench**: Enhancing and Benchmarking Real-world Repository-level Code Completion Abilities of Code Large Language Models [2024-06-arXiv] [[ðŸ“„ paper](https://arxiv.org/abs/2406.01359)]

- **RepoClassBench**: Class-Level Code Generation from Natural Language Using Iterative, Tool-Enhanced Reasoning over Repository [2024-05-arXiv] [[ðŸ“„ paper](https://arxiv.org/abs/2405.01573)] [[ðŸ”— repo](https://github.com/microsoft/repoclassbench/tree/main)]

- **DevEval**: Evaluating Code Generation in Practical Software Projects [2024-ACL-Findings] [[ðŸ“„ paper](https://aclanthology.org/2024.findings-acl.214.pdf)] [[ðŸ”— repo](https://github.com/seketeam/DevEval)]

- **CodAgentBench**: Enhancing Code Generation with Tool-Integrated Agent Systems for Real-World Repo-level Coding Challenges [2024-ACL] [[ðŸ“„ paper](https://aclanthology.org/2024.acl-long.737/)]

- **RepoBench**: Benchmarking Repository-Level Code Auto-Completion Systems [2024-ICLR] [[ðŸ“„ paper](https://openreview.net/forum?id=pPjZIOuQuF)] [[ðŸ”— repo](https://github.com/Leolty/repobench)]

- **SWE-bench**: Can Language Models Resolve Real-World GitHub Issues? [2024-ICLR] [[ðŸ“„ paper](https://arxiv.org/pdf/2310.06770)] [[ðŸ”— repo](https://github.com/princeton-nlp/SWE-bench)]

- **CrossCodeLongEval**: Repoformer: Selective Retrieval for Repository-Level Code Completion [2024-ICML] [[ðŸ“„ paper](https://arxiv.org/abs/2403.10059)] [[ðŸ”— repo](https://repoformer.github.io/)]

- **R2E-Eval**: Turning Any GitHub Repository into a Programming Agent Test Environment [2024-ICML] [[ðŸ“„ paper](https://proceedings.mlr.press/v235/jain24c.html)] [[ðŸ”— repo](https://r2e.dev/)]

- **RepoEval**: Repository-Level Code Completion Through Iterative Retrieval and Generation [2023-EMNLP] [[ðŸ“„ paper](https://aclanthology.org/2023.emnlp-main.151/)] [[ðŸ”— repo](https://github.com/microsoft/CodeT/tree/main/RepoCoder)]

- **CrossCodeEval**: A Diverse and Multilingual Benchmark for Cross-File Code Completion [2023-NeurIPS] [[ðŸ“„ paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/920f2dced7d32ab2ba2f1970bc306af6-Paper-Datasets_and_Benchmarks.pdf)] [[ðŸ”— site](https://crosscodeeval.github.io/)]